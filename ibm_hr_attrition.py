# -*- coding: utf-8 -*-
"""IBM_HR_Attrition.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LZoYxbMrfT-nIYaisGAMApfQDiIKiGXg

# IBM HR Analytics Employee Attrition & Performance
"""



"""# Reading and understanding the data"""

import pandas as pd

data = pd.read_csv("/content/WA_Fn-UseC_-HR-Employee-Attrition.csv")
data

data.value_counts()

data.columns

data.info()

data.describe()

data.shape

#Duplicated Data
print("Number of Duplicated Data :",data.duplicated().sum())

#Null value data
print("Null Value Data:",data.isnull().sum())

data.dtypes

# Categorical columns
cat_col = [col for col in data.columns if data[col].dtype == 'object']
print('Categorical columns :',cat_col)
# Numerical columns
num_col = [col for col in data.columns if data[col].dtype != 'object']
print('Numerical columns :',num_col)

data.nunique()

"""# Exploratory Data Analysis

"""

import matplotlib.pyplot as plt
import seaborn as sns

data.columns

data['Attrition']

#Min and Max values of age amongst employees
print("Minimum Age of Employees is :", data['Age'].min())
print("Maximum Age of Employees is :",data['Age'].max())

#Age with Attrition

plt.figure(figsize=(16, 6))

plt.subplot(1, 2, 1)
sns.boxplot(x='Attrition', y='Age', data=data)
plt.title('Age Distribution with Attrition')

plt.subplot(1, 2, 2)
plt.grid()
sns.histplot(data['Age'], kde=True, color = "Green")
plt.title('Age Distribution')


plt.tight_layout()
plt.show()

"""1. **It can be observed that the maximum employess are in the age range between  25-40 years.The minimum age of an employee is 18 yrs , while the maximum age is 60 yrs.**
2. **From Attrition it is interpreted that that the employees leaving the company are below the age of 40 yrs, while the employees who didn't leave the company lie in the range of 32-42 yrs of age.**  
"""

data['DistanceFromHome'].value_counts()

#Distance from Home

plt.figure(figsize=(16, 6))

plt.subplot(1,2,1)
sns.boxplot(x='Attrition', y='DistanceFromHome', data=data,color = "Yellow")
plt.title('Distance From Home vs. Attrition')

plt.subplot(1,2,2)
sns.histplot(data['DistanceFromHome'],kde=True,color = "Red")
plt.title('Distance From Home Distribution')

plt.tight_layout()
plt.show()

"""1. **Maximum employees reside near the office in the distance range of 0-10 km.**
2. **It can be observed that as the distance from home increases the attrition levels also increases , while less distance ensures less attrition level.**
"""

#Attrition with Monthly Income

plt.figure(figsize=(16, 6))

plt.subplot(1, 2, 1)
sns.boxplot(x='Attrition', y='MonthlyIncome', data=data)
plt.title('Monthly Income vs. Attrition')

plt.subplot(1,2,2)
sns.histplot(data['MonthlyIncome'],kde=True,color = "Green")
plt.title('Monthly Income Distribution')

plt.tight_layout()
plt.show()

"""1. **It can be observed that when the monthly income is below 6000, the attrition levels are high .**
2. **While increase in the monthly income shows less attrition level.**
3. **Also the outlier values , indicate that no attrition is observed when the monthly income increases above 17500, which is evident to say that more attrition is observed with less monthly income and vice versa**
"""

data['TotalWorkingYears'].value_counts()

#Attrition with Total Working Years

plt.figure(figsize=(16, 6))

plt.subplot(1, 2, 1)
sns.boxplot(x='Attrition', y='TotalWorkingYears', data=data,color = "Yellow")
plt.title('Attrition vs No.of Working Years')

plt.subplot(1,2,2)
sns.histplot(data['TotalWorkingYears'], kde=True, color = "Red")
plt.title('Total Working Years Distribution')

plt.tight_layout()
plt.show()

"""1. **It can be observed that during the initial years from 2-10 years , the attrition levels are High.**
2. **But With an increase in the number of working years the attrition levels decreases.**
"""

data['PercentSalaryHike'].value_counts()

#Attrition with Percent Salart Hike

plt.figure(figsize=(16, 6))

plt.subplot(1, 2, 1)
sns.boxplot(x='Attrition', y='PercentSalaryHike', data=data)
plt.title('Percent Salary Hike with Attrition')

plt.subplot(1, 2, 2)
plt.grid()
sns.histplot(data['PercentSalaryHike'], kde=True, color = "Green")
plt.title('PercentSalaryHike Distribution')

plt.tight_layout()
plt.show()

"""1. **Attrition can be observed in the range of 12% to 17% Salary Hike.**
2. **However with an increase in the salary hike from 17% to near 18% there is a decrease in the attrition levels.**
"""

data['NumCompaniesWorked'].value_counts()

#Attrition with Number of Companies worked with
plt.figure(figsize=(16, 6))

plt.subplot(1, 2, 1)
sns.boxplot(x='Attrition', y='NumCompaniesWorked', data=data)
plt.title('Number of Companies worked with Attrition')

plt.subplot(1, 2, 2)
plt.grid()
sns.histplot(data['NumCompaniesWorked'], kde=True, color = "Red")
plt.title('PercentSalaryHike Distribution')


plt.tight_layout()
plt.show()

"""1. **When the number of Companies worked is between the range of 1-5 , attrition is observed to be high.**
2. **While when the range is between 1-4 the attrition levels are low.**
"""

data['YearsInCurrentRole'].value_counts()

#Years in Current Role With Attrition

plt.figure(figsize=(16, 6))

plt.subplot(1, 2, 1)
sns.boxplot(x='Attrition', y='YearsInCurrentRole', data=data)
plt.title('Years in Current Role with Attrition')

plt.subplot(1, 2, 2)
plt.grid()
sns.histplot(data['YearsInCurrentRole'], kde=True, color = "Green")
plt.title('Years In Current Role Distribution')

plt.tight_layout()
plt.show()

"""1. **Higher Attrition rates are observed in initial 0-4 yrs.**
2. **However once the no. of years at the current role starts increasing the attrition levels also decrease.**
"""

data['PerformanceRating'].value_counts()
#1 Low
#2 Good
#3 Excellent
#4 Outstanding

import pandas as pd

# Filter for employees with JobInvolvement equal to 1
perf_rating_3 = data[data['PerformanceRating'] == 3]
perf_rating_4 = data[data['PerformanceRating'] == 4]


# Count the number of 'Yes' values in the 'Attrition' column for those employees
attrition_yes_count = perf_rating_3['Attrition'].value_counts().get('Yes', 0)
attrition_yes_count_4 = perf_rating_4['Attrition'].value_counts().get('Yes', 0)

print(f"Number of employees with Attrition Yes in rating 3: {attrition_yes_count}")
print(f"Number of employees with Attrition Yes in rating 4 : {attrition_yes_count_4}")

#Performance rating with Attrition

plt.figure(figsize=(16, 6))

plt.subplot(1, 2, 1)
sns.countplot(y='PerformanceRating', hue='Attrition', data=data)
plt.title('Performance Rating Distribution with Attrition')

plt.subplot(1, 2, 2)
plt.pie(data['PerformanceRating'].value_counts(), labels=data['PerformanceRating'].value_counts().index, autopct='%1.1f%%')
plt.title('Work Life Balance Distribution')

plt.tight_layout()
plt.show()

"""1. **With the above observations, 16% attrition can be observed in both  excellent rating (3), and Outstanding rating (4).**

"""

data['WorkLifeBalance'].value_counts()
#1 Bad
#2 Good
#3 Better
#4 Best

Work_life_2 = data[data['WorkLifeBalance'] == 2]
Work_life_3 = data[data['WorkLifeBalance'] == 3]

# Count the number of 'Yes' values in the 'Attrition' column for those employees
attrition_yes_count = Work_life_2['Attrition'].value_counts().get('Yes', 0)
attrition_yes_count_3 = Work_life_3['Attrition'].value_counts().get('Yes', 0)

print(f"Number of employees with Attrition Yes in rating 2: {attrition_yes_count}")
print(f"Number of employees with Attrition Yes in rating 3 : {attrition_yes_count_3}")

#Work Life Balance with Attrition

plt.figure(figsize=(16, 6))

plt.subplot(1, 2, 1)
sns.countplot(y='WorkLifeBalance', hue='Attrition', data=data)
plt.title('Work Life Balance Distribution with Attrition')

plt.subplot(1, 2, 2)
plt.pie(data['WorkLifeBalance'].value_counts(), labels=data['WorkLifeBalance'].value_counts().index, autopct='%1.1f%%')
plt.title('Work Life Balance Distribution')

plt.tight_layout()
plt.show()

"""1. **The above distribution indicates that maximum employees experience good or better Work Life Balance.**
2. **Similarly Higher attrition levels are observed in good with 16% and 14% for better work life balance.**
"""

data['EnvironmentSatisfaction'].value_counts()

#Environment Satisfaction with Attrition
plt.figure(figsize=(16, 6))

plt.subplot(1, 2, 1)
sns.countplot(y='EnvironmentSatisfaction', hue='Attrition', data=data)
plt.title('Environment Satisfaction Distribution with Attrition')

plt.subplot(1, 2, 2)
plt.pie(data['EnvironmentSatisfaction'].value_counts(), labels=data['EnvironmentSatisfaction'].value_counts().index, autopct='%1.1f%%')
plt.title('Environment Satisfaction Distribution')

plt.tight_layout()
plt.show()

"""1. **It is observed that people with Low environment satisfaction (25%) leave the company.**"""

data['JobInvolvement'].value_counts()

import pandas as pd

# Filter for employees with JobInvolvement equal to 1
job_involvement_1 = data[data['JobInvolvement'] == 1]
job_involvement_4 = data[data['JobInvolvement'] == 4]


# Count the number of 'Yes' values in the 'Attrition' column for those employees
attrition_yes_count = job_involvement_1['Attrition'].value_counts().get('Yes', 0)
attrition_yes_count_4 = job_involvement_4['Attrition'].value_counts().get('Yes', 0)

print(f"Number of employees with Job Involvement 1 and Attrition Yes: {attrition_yes_count}")
print(f"Number of employees with Job Involvement 4 and Attrition Yes : {attrition_yes_count_4}")

#Job Involvement with Attrition
plt.figure(figsize=(16, 6))

attrition_by_job_involvement = data.groupby(['JobInvolvement', 'Attrition']).size().unstack()
attrition_by_job_involvement.plot(kind='bar', stacked=True)
plt.title('Attrition by Job Involvement')
plt.xlabel('Job involvement')
plt.ylabel('Number of Employees')
plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better readability
plt.legend(title='Attrition')
plt.show()

plt.pie(data['JobInvolvement'].value_counts(), labels=data['JobInvolvement'].value_counts().index, autopct='%1.1f%%')
plt.title('Job Involvement Distribution')

plt.show()

"""1. **From the total, 59% employees have high job involvement, while only 25% have medium job involvement.**
2. **It is seen that maximum attrition can be observed in the moderate Job Involvement (2 & 3).**
3. **While only 9% attrition is observed in the High Job_Involvement(4) and 33% attrition in Low Job Involvement (1).**
4. **This indicates that as the job_involvement is high , attrition is low and vice versa.**
"""

data['JobSatisfaction'].value_counts()

import pandas as pd



# Filter for employees with JobInvolvement equal to 1
job_satis_1 = data[data['JobSatisfaction'] == 1]
job_satis_4 = data[data['JobSatisfaction'] == 4]
job_satis_3 = data[data['JobSatisfaction'] == 3]

# Count the number of 'Yes' values in the 'Attrition' column for those employees
attrition_yes_count = job_satis_1['Attrition'].value_counts().get('Yes', 0)
attrition_yes_count_4 = job_satis_4['Attrition'].value_counts().get('Yes', 0)
attrition_yes_count_3 = job_satis_3['Attrition'].value_counts().get('Yes', 0)

print(f"Number of employees with Job Satisfaction 1 and Attrition Yes: {attrition_yes_count}")
print(f"Number of employees with Job Satisfaction 4 and Attrition Yes : {attrition_yes_count_4}")
print(f"Number of employees with Job Satisfaction 3 and Attrition Yes : {attrition_yes_count_3}")

#Job Satisfaction with Attrition

plt.figure(figsize=(16, 6))

attrition_by_job_satisfaction = data.groupby(['JobSatisfaction', 'Attrition']).size().unstack()
attrition_by_job_satisfaction.plot(kind='bar', stacked=True)
plt.title('Attrition by Job Satisfaction')
plt.xlabel('Job Satisfaction')
plt.ylabel('Number of Employees')
plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better readability
plt.legend(title='Attrition')
plt.show()


plt.pie(data['JobSatisfaction'].value_counts(), labels=data['JobSatisfaction'].value_counts().index, autopct='%1.1f%%')
plt.title('Job Satisfaction Distribution')
plt.show()

"""1. **The employees with low job satisfaction (1) have 23% attrition.**
2. **But what is suprising is , that employees with moderate or high attrition also have a attrition level of 16% and 11% respectively.**
3. **This indicates that apart from Job Satisfaction there are other factors as well that affect that affect attrition.**
"""

data['Department'].nunique()

#Department with Attrition
plt.figure(figsize=(16, 6))

plt.subplot(1, 2, 1)
sns.countplot(x='Department', hue='Attrition', data=data)
plt.title('Department Distribution with Attrition')

plt.subplot(1, 2, 2)
plt.pie(data['Department'].value_counts(), labels=data['Department'].value_counts().index, autopct='%1.1f%%')
plt.title('Department Distribution')


plt.tight_layout()
plt.show()

"""1. **From the above diagrams it can be inferred that highest attrition is observed in the Reasearch and Development Department.**
2. **While The least in Human Rescource department.**
3. **There is proper ratio between the percent of department distribution and its attrition respectively.**


"""

data['Education'].nunique()

data['Education'].value_counts()

import pandas as pd

# Filter for employees with JobInvolvement equal to 1
edu_1 = data[data['Education'] == 3]


# Count the number of 'Yes' values in the 'Attrition' column for those employees
attrition_yes_count = edu_1['Attrition'].value_counts().get('Yes', 0)


print(f"Number of employees with Education 3 and Attrition Yes: {attrition_yes_count}")

#1 Below College
#2 College
#3 Bachelor
#4 Master
#5 Doctor

#Education with Attrition

plt.figure(figsize=(16, 6))

plt.subplot(1,2,1)
sns.countplot(y='Education', hue='Attrition', data=data)
plt.title('Education Distribution with Attrition')

plt.subplot(1, 2, 2)
plt.pie(data['Education'].value_counts(), labels=data['Education'].value_counts().index, autopct='%1.1f%%')
plt.title('Education Distribution')

plt.tight_layout()
plt.show()

"""1. **Here it is observed that maximum attrition is witnessed in Bachelor's(3) with 17% Attrition**
2. **While Lower attrition rates are observed for Master's(4) and Doctor(5).**
"""

data['EducationField'].value_counts()

#Education Field with Attrition

plt.figure(figsize=(16, 6))

plt.subplot(1, 2, 1)
sns.countplot(y='EducationField', hue='Attrition', data=data)
plt.title('Education Field Distribution with Attrition')

plt.subplot(1, 2, 2)
plt.pie(data['EducationField'].value_counts(), labels=data['EducationField'].value_counts().index, autopct='%1.1f%%')
plt.title('Education Field Distribution')

plt.tight_layout()
plt.show()

"""1. **The chart indicates that maximum attrition is in the field of Life Sciences, while all others have lower attrition rates.**"""

data['Gender'].value_counts()

gender_1 = data[data['Gender'] == 'Male']
gender_2 = data[data['Gender'] == 'Female']

# Count the number of 'Yes' values in the 'Attrition' column for those employees
attrition_yes_count = gender_1['Attrition'].value_counts().get('Yes', 0)
attrition_yes_count_2 = gender_2['Attrition'].value_counts().get('Yes', 0)

print(f"Males and Attrition Yes: {attrition_yes_count}")
print(f"Females and Attrition Yes:{attrition_yes_count_2}")

# Gender with Attrition

plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
sns.countplot(y='Gender', hue='Attrition', data=data)
plt.title('Gender Distribution with Attrition')

plt.subplot(1, 2, 2)
plt.pie(data['Gender'].value_counts(), labels=data['Gender'].value_counts().index, autopct='%1.1f%%')
plt.title('Gender Distribution')

plt.tight_layout()
plt.show()

"""1.**Males have 17% attrition, while females have 14% attrition .This indicates that attrition among males is more than females.**"""

data['JobRole'].value_counts()

#Job Role with Attrition
plt.figure(figsize=(16, 6))
plt.subplot(1,2,1)
sns.countplot(y='JobRole', hue='Attrition', data=data)
plt.title('Job Role Distribution with Attrition')

plt.subplot(1, 2, 2)
plt.pie(data['JobRole'].value_counts(), labels=data['JobRole'].value_counts().index, autopct='%1.1f%%')
plt.title('Job Role Distribution')

plt.tight_layout()
plt.show()

"""1. **Here it is observed that in Job Role Distribution , the highest ocuupancy is by Sales Executive, while Human Resource having the lowest occupancy.**
2.**With respect to that higher attrition rates are observed in Sales Executive, Laboratory technician and so.**
"""

data['MaritalStatus'].value_counts()

#Marital Status with Attrition

plt.figure(figsize=(16, 6))
plt.subplot(1,2,1)
sns.countplot(y='MaritalStatus', hue='Attrition', data=data)
plt.title('Marital Status Distribution with Attrition')

plt.subplot(1, 2, 2)
plt.pie(data['MaritalStatus'].value_counts(), labels=data['MaritalStatus'].value_counts().index, autopct='%1.1f%%')
plt.title('Marital Status Distribution')

plt.tight_layout()
plt.show()

"""1.**Though the distribution of Married is high, in maritial status , higher attrition is observed in single, while lowest in divorced.**"""

data['OverTime'].value_counts()

#Overtime with Attrition
plt.figure(figsize=(16, 6))
plt.subplot(1,2,1)
sns.countplot(y='OverTime', hue='Attrition', data=data)
plt.title('Over Time Distribution with Attrition')

plt.subplot(1, 2, 2)
plt.pie(data['OverTime'].value_counts(), labels=data['OverTime'].value_counts().index, autopct='%1.1f%%')
plt.title('Over Time Distribution')

plt.tight_layout()
plt.show()

"""# Feature Engineering and Selection

1. **Dropping Irrelevant Columns that don't provide useful predictive value and should be removed.**
"""

#Dropping irrelevant columns

data.drop(['EmployeeCount','EmployeeNumber','Over18','StandardHours'],axis=1,inplace=True)

data['JobRole'].nunique()

"""2. **Handling Categorical Variables.**"""

from sklearn.preprocessing import LabelEncoder

# Define categorical variables
Nominal_Categorical = ['BusinessTravel', 'Department', 'EducationField', 'Gender', 'JobRole', 'MaritalStatus', 'OverTime']
Ordinal_Categorical = ['Attrition', 'Education', 'EnvironmentSatisfaction', 'JobInvolvement',
                       'JobSatisfaction', 'JobLevel', 'PerformanceRating', 'RelationshipSatisfaction', 'WorkLifeBalance']

# One-Hot Encoding (Creates new columns for each category)
df = pd.get_dummies(data=data, columns=Nominal_Categorical)

# Label Encoding (For ordinal categorical variables)
label_encoder = LabelEncoder()
for col in Ordinal_Categorical:
    df[col] = label_encoder.fit_transform(df[col])

# Display first few rows to check changes
df.head()

"""
3. **Apply SMOTE (Synthetic Minority Over-sampling Technique) to handle class imbalance**"""

from sklearn.model_selection import train_test_split

X = df.drop(columns=['Attrition'])  # Features
y = df['Attrition'] # Target variable

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

from imblearn.over_sampling import SMOTE

smote = SMOTE(sampling_strategy='auto', random_state=42)
X_train, y_train = smote.fit_resample(X_train, y_train)

# Check class balance before and after SMOTE
print("Smote Transformation:", pd.Series(y_train).value_counts())

"""# Model Selection and Development

## **Logistic Regression**
"""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report

lr_model = LogisticRegression()
lr_model.fit(X_train, y_train)

y_pred_lr = lr_model.predict(X_test)
print("Logistic Regression Accuracy:", accuracy_score(y_test, y_pred_lr))
print(classification_report(y_test, y_pred_lr))

"""### **Hyperparameter Tuning**"""

from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LogisticRegression

# Define the Logistic Regression model
log_reg = LogisticRegression()

param_grid = {
    'C': [0.001, 0.01, 0.1, 1, 10, 100],  # Regularization strength
    'solver': ['liblinear', 'lbfgs']  # Solvers that support L2 regularization
}

# Apply GridSearchCV
grid_search = GridSearchCV(log_reg, param_grid, cv=5, scoring='accuracy', n_jobs=-1)
grid_search.fit(X_train,y_train)

# Print best parameters
print("Best parameters:", grid_search.best_params_)

# Get best model
best_log_reg = grid_search.best_estimator_

# Evaluate on test data
y_pred = best_log_reg.predict(X_test)
from sklearn.metrics import classification_report, accuracy_score

print("Optimized Logistic Regression Accuracy:", accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred))

"""## **Decision Tree and Random Forest**"""

from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier

# Decision Tree
dt_model = DecisionTreeClassifier(random_state=42)
dt_model.fit(X_train, y_train)

y_pred_dt = dt_model.predict(X_test)
print("Decision Tree Accuracy:", accuracy_score(y_test, y_pred_dt))

# Random Forest
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

y_pred_rf = rf_model.predict(X_test)
print("Random Forest Accuracy:", accuracy_score(y_test, y_pred_rf))

"""### **Hyperparameter Tuning**"""

from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import RandomizedSearchCV
import numpy as np

# Define models
dt = DecisionTreeClassifier(random_state=42)
rf = RandomForestClassifier(random_state=42)

# Hyperparameter grids
dt_param_grid = {
    'max_depth': [3, 5, 10, 20, None],
    'min_samples_split': [2, 5, 10, 20],
    'min_samples_leaf': [1, 2, 5, 10],
    'criterion': ['gini', 'entropy']
}

rf_param_grid = {
    'n_estimators': [50, 100, 200, 300],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'bootstrap': [True, False]
}

dt_search = RandomizedSearchCV(dt, dt_param_grid, cv=5, scoring='accuracy', n_iter=10, random_state=42, n_jobs=-1)
rf_search = RandomizedSearchCV(rf, rf_param_grid, cv=5, scoring='accuracy', n_iter=10, random_state=42, n_jobs=-1)

dt_search.fit(X_train, y_train)
rf_search.fit(X_train ,y_train)

# Print best parameters
print("Best parameters for Decision Tree:", dt_search.best_params_)
print("Best parameters for Random Forest:", rf_search.best_params_)

# Get best models
best_dt = dt_search.best_estimator_
best_rf = rf_search.best_estimator_

from sklearn.metrics import classification_report, accuracy_score

y_pred_dt = best_dt.predict(X_test)
y_pred_rf = best_rf.predict(X_test)

print("Optimized Decision Tree Accuracy:", accuracy_score(y_test, y_pred_dt))
print(classification_report(y_test, y_pred_dt))

print("Optimized Random Forest Accuracy:", accuracy_score(y_test, y_pred_rf))
print(classification_report(y_test, y_pred_rf))

"""## XGBoost"""

from xgboost import XGBClassifier

xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss')
xgb_model.fit(X_train, y_train)

y_pred_xgb = xgb_model.predict(X_test)
print("XGBoost Accuracy:", accuracy_score(y_test, y_pred_xgb))

"""### Hyperparameter Tuning"""

from xgboost import XGBClassifier
from sklearn.model_selection import RandomizedSearchCV

# Define XGBoost model
xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)

# Hyperparameter grid
xgb_param_grid = {
    'n_estimators': [50, 100, 200, 300],
    'learning_rate': [0.01, 0.05, 0.1, 0.2],
    'max_depth': [3, 5, 10, 15],
    'subsample': [0.6, 0.8, 1.0],  # Fraction of samples used for training
    'colsample_bytree': [0.6, 0.8, 1.0],  # Fraction of features used for each tree
    'gamma': [0, 0.1, 0.2, 0.3],  # Minimum loss reduction for a split
    'reg_lambda': [0, 1, 10],  # L2 regularization (prevents overfitting)
}

# Apply RandomizedSearchCV
xgb_search = RandomizedSearchCV(xgb, xgb_param_grid, cv=5, scoring='accuracy', n_iter=10, random_state=42, n_jobs=-1)
xgb_search.fit(X_train, y_train)

# Print best parameters
print("Best parameters for XGBoost:", xgb_search.best_params_)

# Get best model
best_xgb = xgb_search.best_estimator_

# Evaluate on test data
from sklearn.metrics import classification_report, accuracy_score

y_pred_xgb = best_xgb.predict(X_test)

print("Optimized XGBoost Accuracy:", accuracy_score(y_test, y_pred_xgb))
print(classification_report(y_test, y_pred_xgb))

"""# Model Evaluation"""

from sklearn.metrics import confusion_matrix, roc_auc_score

print("Random Forest AUC Score:", roc_auc_score(y_test, y_pred_rf))
print("XGBoost AUC Score:", roc_auc_score(y_test, y_pred_xgb))

from sklearn.metrics import classification_report, accuracy_score

# Predictions for all models
y_pred_logreg = best_log_reg.predict(X_test)
y_pred_dt = best_dt.predict(X_test)
y_pred_rf = best_rf.predict(X_test)
y_pred_xgb = best_xgb.predict(X_test)

def evaluate_model(model_name, y_true, y_pred):
    print(f"\n🔹 {model_name} Evaluation:")
    print(f"Accuracy: {accuracy_score(y_true, y_pred):.4f}")
    print(classification_report(y_true, y_pred))


evaluate_model("Logistic Regression", y_test, y_pred_logreg)
evaluate_model("Decision Tree", y_test, y_pred_dt)
evaluate_model("Random Forest", y_test, y_pred_rf)
evaluate_model("XGBoost", y_test, y_pred_xgb)

evaluation_results = {
    "Model": ["Logistic Regression", "Decision Tree", "Random Forest", "XGBoost"],
    "Accuracy": [
        accuracy_score(y_test, y_pred_logreg),
        accuracy_score(y_test, y_pred_dt),
        accuracy_score(y_test, y_pred_rf),
        accuracy_score(y_test, y_pred_xgb),
    ]
}

df_results = pd.DataFrame(evaluation_results).sort_values(by="Accuracy", ascending=False)
print("\nModel Comparison:\n", df_results)

"""1. From the above model results , it is evident that :

* ✅ XGBoost is the best choice for deployment due to its balance of accuracy and generalization.
* ✅ Logistic Regression is still valuable for interpretability and baseline comparisons.
* ✅ Random Forest provides a strong alternative but can be computationally expensive.
* ✅ Decision Tree needs further tuning to reduce overfitting and improve performance.


"""